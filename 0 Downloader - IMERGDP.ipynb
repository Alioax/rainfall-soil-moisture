{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0feca41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import time\n",
    "import netrc\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c11a2cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "LINKS_FILE = \"./data/IMERGDP links.txt\"\n",
    "PROCESSED_DIR = Path(\"./data/processed/IMERGDP/Granules\")\n",
    "RAW_DIR = Path(\"./data/raw/IMERGDP\")\n",
    "\n",
    "\n",
    "if RAW_DIR.exists():\n",
    "    for f in RAW_DIR.iterdir():\n",
    "        if f.is_file():\n",
    "            f.unlink()\n",
    "        elif f.is_dir():\n",
    "            shutil.rmtree(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0e2a7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_imerg_file(file_path):\n",
    "    \"\"\"\n",
    "    Process a single IMERG file and extract precipitation statistics for the area of interest.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the IMERG NC4 file\n",
    "        \n",
    "    Returns:\n",
    "        None: Saves the results to a CSV file in the processed directory\n",
    "    \"\"\"\n",
    "    # Open the dataset\n",
    "    ds = xr.open_dataset(file_path)\n",
    "    \n",
    "    # Define the bounding box coordinates\n",
    "    lat_min, lat_max = 30.34000, 30.56000\n",
    "    lon_min, lon_max = -91.28000, -91.02000\n",
    "    \n",
    "    # Subset the dataset to the bounding box\n",
    "    ds_subset = ds.sel(lat=slice(lat_min, lat_max), \n",
    "                      lon=slice(lon_min, lon_max))\n",
    "    \n",
    "    # Calculate mean precipitation over the spatial subset\n",
    "    mean_precip = float(ds_subset['precipitation'].mean(dim=['lat', 'lon']))\n",
    "    \n",
    "    # Count valid precipitation values (not NaN and not -9999)\n",
    "    valid_count = int(((ds_subset['precipitation'] != -9999) & \n",
    "                      (~np.isnan(ds_subset['precipitation']))).sum())\n",
    "    \n",
    "    # Get the date from the dataset attributes\n",
    "    date_str = ds_subset.attrs.get('BeginDate') or ds_subset.attrs.get('EndDate')\n",
    "    if date_str:\n",
    "        date = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "        formatted_date = date.strftime('%Y-%m-%d')\n",
    "    else:\n",
    "        # If date not in attributes, try to get it from time coordinate\n",
    "        if 'time' in ds_subset.coords:\n",
    "            date = ds_subset.time.values[0]\n",
    "            formatted_date = pd.Timestamp(date).strftime('%Y-%m-%d')\n",
    "        else:\n",
    "            formatted_date = \"Date not found\"\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'date': [date],\n",
    "        'precipitation': [mean_precip],\n",
    "        'valid_count': [valid_count]\n",
    "    })\n",
    "    \n",
    "    # Create output filename based on input filename\n",
    "    filename = os.path.basename(file_path)\n",
    "    output_filename = os.path.splitext(filename)[0] + '.csv'\n",
    "    output_path = os.path.join(PROCESSED_DIR, output_filename)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_path, index=False)\n",
    "    \n",
    "    # Close the dataset\n",
    "    ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2d63d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed before start: 1096 | Processed this run: 0 | Total processed: 1096 / Required total: 1096\n",
      "---------------------------------------------------------------------------------------------------\n",
      "All files processed!\n"
     ]
    }
   ],
   "source": [
    "import os, tempfile, time, netrc, requests\n",
    "from urllib.parse import urlparse\n",
    "from pathlib import Path\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# --- your process_imerg_file(...) goes here ---\n",
    "\n",
    "def _urs_auth_from_netrc():\n",
    "    \"\"\"Read Earthdata credentials from ~/.netrc\"\"\"\n",
    "    try:\n",
    "        n = netrc.netrc()\n",
    "        host = \"urs.earthdata.nasa.gov\"\n",
    "        auth = n.authenticators(host)\n",
    "        if auth and auth[0] and auth[2]:\n",
    "            from requests.auth import HTTPBasicAuth\n",
    "            return HTTPBasicAuth(auth[0], auth[2])\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def make_header(already, processed_in_run, total, overall):\n",
    "    return (f\"Processed before start: {already} | \"\n",
    "            f\"Processed this run: {processed_in_run} | \"\n",
    "            f\"Total processed: {overall} / Required total: {total}\")\n",
    "\n",
    "def _render_status(header_text, body_lines):\n",
    "    clear_output(wait=True)\n",
    "    print(header_text)\n",
    "    print(\"-\" * len(header_text))\n",
    "    for line in body_lines:\n",
    "        print(line)\n",
    "\n",
    "def _download_with_requests(url: str, out_path: Path, auth, get_header):\n",
    "    out_tmp = out_path.with_suffix(out_path.suffix + \".part\")\n",
    "    with requests.Session() as s:\n",
    "        s.auth = auth\n",
    "        with s.get(url, stream=True, timeout=60, allow_redirects=True) as r:\n",
    "            r.raise_for_status()\n",
    "            total = int(r.headers.get(\"content-length\", 0))\n",
    "            got = 0\n",
    "            chunk = 1024 * 256\n",
    "            start = time.time()\n",
    "            with open(out_tmp, \"wb\") as f:\n",
    "                for b in r.iter_content(chunk_size=chunk):\n",
    "                    if not b:\n",
    "                        continue\n",
    "                    f.write(b)\n",
    "                    got += len(b)\n",
    "                    pct = (got / total * 100) if total else 0\n",
    "                    speed = got / max(1e-6, (time.time() - start))\n",
    "                    body = [\n",
    "                        f\"DOWNLOADING {out_path.name}\",\n",
    "                        (f\"Progress: {pct:6.2f}%  ({got:,}/{total:,} bytes)  |  \"\n",
    "                         f\"{speed/1e6:,.2f} MB/s\") if total else f\"Progress: {got:,} bytes\",\n",
    "                        \"PROCESSING: pending\",\n",
    "                        \"NEXT FILE: pending\",\n",
    "                    ]\n",
    "                    _render_status(get_header(), body)\n",
    "    out_tmp.replace(out_path)\n",
    "\n",
    "def download_and_process_imerg():\n",
    "    PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    processed_set = {p.stem for p in PROCESSED_DIR.glob(\"*.csv\")}\n",
    "    with open(LINKS_FILE, \"r\") as f:\n",
    "        urls = [ln.strip() for ln in f if ln.strip() and not ln.strip().startswith(\"#\")]\n",
    "\n",
    "    already = len(processed_set)\n",
    "    total_required = len(urls)\n",
    "    processed_in_run = 0\n",
    "\n",
    "    def get_header():\n",
    "        overall = len(processed_set)\n",
    "        return make_header(already, processed_in_run, total_required, overall)\n",
    "\n",
    "    # initial header\n",
    "    _render_status(get_header(), [\"Waiting…\"])\n",
    "\n",
    "    auth = _urs_auth_from_netrc()\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(prefix=\"earthdata_cookies_\", delete=False) as cj:\n",
    "        cookiejar = Path(cj.name)\n",
    "    try:\n",
    "        for url in urls:\n",
    "            filename = Path(urlparse(url).path).name\n",
    "            basename = Path(filename).stem\n",
    "\n",
    "            if basename in processed_set:\n",
    "                _render_status(get_header(), [\n",
    "                    f\"SKIPPING {basename} (already processed)\",\n",
    "                    \"DOWNLOADING: skipped\",\n",
    "                    \"PROCESSING: skipped\",\n",
    "                    \"DONE\",\n",
    "                    \"NEXT FILE\",\n",
    "                ])\n",
    "                time.sleep(0.01)\n",
    "                continue\n",
    "\n",
    "            raw_path = RAW_DIR / filename\n",
    "\n",
    "            # Download with live progress\n",
    "            try:\n",
    "                _download_with_requests(url, raw_path, auth, get_header)\n",
    "            except Exception as e:\n",
    "                _render_status(get_header(), [\n",
    "                    f\"DOWNLOADING {filename}: ERROR -> {e}\",\n",
    "                    \"PROCESSING: skipped\",\n",
    "                    \"NEXT FILE\",\n",
    "                ])\n",
    "                time.sleep(0.1)\n",
    "                continue\n",
    "\n",
    "            # Process\n",
    "            _render_status(get_header(), [\n",
    "                f\"DOWNLOADING {filename}: 100.00% (saved)\",\n",
    "                f\"PROCESSING {filename} …\",\n",
    "                \"NEXT FILE: pending\",\n",
    "            ])\n",
    "            try:\n",
    "                process_imerg_file(str(raw_path))\n",
    "                raw_path.unlink(missing_ok=True)\n",
    "                processed_set.add(basename)\n",
    "                processed_in_run += 1\n",
    "                _render_status(get_header(), [\n",
    "                    f\"DOWNLOADING {filename}: 100.00% (saved)\",\n",
    "                    f\"PROCESSING {filename}: done\",\n",
    "                    \"DONE\",\n",
    "                    \"NEXT FILE\",\n",
    "                ])\n",
    "            except Exception as e:\n",
    "                raw_path.unlink(missing_ok=True)\n",
    "                _render_status(get_header(), [\n",
    "                    f\"DOWNLOADING {filename}: 100.00% (saved)\",\n",
    "                    f\"PROCESSING {filename}: ERROR -> {e}\",\n",
    "                    \"NEXT FILE\",\n",
    "                ])\n",
    "            time.sleep(0.1)\n",
    "\n",
    "        _render_status(get_header(), [\"All files processed!\"])\n",
    "    finally:\n",
    "        cookiejar.unlink(missing_ok=True)\n",
    "\n",
    "# run\n",
    "download_and_process_imerg()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
